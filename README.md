
![sparkfy](https://github.com/sandramalaquias/data-engineering-Redshift/blob/4184288175034ba8a580a25b533a4ed289c9caf5/sparkfy.png)

# Project: Data Warehouse with REDSHIFT

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights into what songs their users are listening to. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.
Project Description

To complete the project, I get access to AWS resources programaticaly using BOTO3 API, loaded data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables and get a single dashboard for songs played. Finally, get disconnect from AWS resources progamatically again using BOTO3 API.


## Project Description

This project on data warehouse was made with REDSHIFT / Postgres defining fact and dimension tables (star schema) for a particular analytic focus. And has an ETL pipeline using Python to transfers data from S3 Bucket to Redshift.

## Files

### Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.

>song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json


And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```
This dataset was distributed into songs table and artist table.

### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![](https://video.udacity-data.com/topher/2019/February/5c6c15e9_log-data/log-data.png)
This dataset was distributed into time, users, and songs_play table.

## Entity Relationship Diagram (ERD) - Star Schema

![REDSHIFT_ERD](https://github.com/sandramalaquias/data-engineering-Redshift/blob/4184288175034ba8a580a25b533a4ed289c9caf5/REDSHIFT_ERD.png)

 **Benefits of the Star Schema**

-   It is extremely simple to understand and build.
-   No need for complex joins when querying data.
-   Accessing data is faster (because the engine doesnâ€™t have to join various tables to generate results).
-   Simpler to derive business insights.
-   Works well with certain tools for analytics, in particular, with OLAP systems that can create OLAP cubes from data stored.


## Run the scripts

The scripts of this project consist in:
1. Get your KEY and SCRET KEY from AWS IAM
2. Update the file "dw1.config", changing the content of the variables that have string "update"
3. Run the script "AWS_connect.py" at console to get the connection to Redshift and S3
4. Run the script "create_tables.py" at console to create REDSHIFT schema and tables to "staging" and "sparkfy"
5. Run the script "etl.py" at console to populate all the tables. This script also create a file with a single dashboard for songsplay
6. Run the script "AWS_disconnect.py" at console to close the AWS resources

![flow](https://github.com/sandramalaquias/data-engineering-Redshift/blob/4184288175034ba8a580a25b533a4ed289c9caf5/AWS.drawio.png)

### The environment:
-  Python versions from 3.6 to 3.10    
-  configparser
-  psycopg2
-  datetime
-  matplotlib
-  seabonr
-  pandas
-  boto3
-  json
-  sql
-  os

## Scripts description

### AWS connect 
To create connection to AWS resources, is needed a AWS KEY and SECRET KEY (user IAM) and allowed programatically access to these resources via API. Do it manually inside your AWS account.

With these information, update "dwh1.config". The script follows steps bellow:
- create a new Rule to allow REDSHIFT cluster to call AWS services on your behalf
- attach policies to the rule:
    - AMAZON S3 READ ONLY ACCESS
    - AMAZON REDSHIFT ALL COMMANDS FULL ACCESS
- access S3 Bucket: Udacity
- create Redshift Cluster
- Open TCP port to access the cluster endpoint (REDSHIFT - port 5439)
- create a S3 Bucket - MyBucket
  - attach policy "Public"
- create a file song_manifest on S3 Bucket - MyBucket
- get connect string to REDSHIFT / Postegree
- update DWH1.CONFIG


### Create table
To create tables use the scripts:
- Statements in `sql_queries.py` to create each schema (staging and sparkfy) and tables.
- The information on "dwh1.config" updated from "AWS_connect.py"

### Run the ETL
To populate the tables use the scripts:
- Statements in `sql_queries.py `to insert records into each table and get some inputs from data
- The information on "dwh1.config" updated from "AWS_connect.py"
- At end, single graph is created and salved on Results.

### AWS desconnect 
To end the, use this script to desallow your AWS resources. The script follows steps bellow:
- Delete AWS REDSHIFT Cluster
- Delete file song_manifest on S3 Bucket - MyBucket
- Delete S3 Bucket - MyBucket
- Deattach policies to the rule:
    - AMAZON S3 READ ONLY ACCESS
    - AMAZON REDSHIFT ALL COMMANDS FULL ACCESS
- Delete IAM Rule created


## Data Analytics

Using this ERD and their tables, is an easier way to create queries to answer the single question to analytics issues like subcribers level.
- In this plot, is possible to see that the level of subscribers (free, paid) has no variation. Maybe intensifying the market campaing to get more paid subscriber, increase the profit.

![level](https://github.com/sandramalaquias/data-engineering-Redshift/blob/4184288175034ba8a580a25b533a4ed289c9caf5/Results/Song_level.png)


Or make more advanced topics like predict user churn or music recommendation.
 
